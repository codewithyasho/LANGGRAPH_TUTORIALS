{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dc94b86",
   "metadata": {},
   "source": [
    "## **Orchestrator-worker Workflow**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "79902673",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict, Literal\n",
    "from langchain.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_ollama import ChatOllama\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Annotated, List\n",
    "import operator\n",
    "from langgraph.types import Send"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6648cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOllama(model=\"deepseek-v3.1:671b-cloud\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06bbac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for structured output to use in planning\n",
    "class Section(BaseModel):\n",
    "    name: str = Field(\n",
    "        description=\"Name for this section of the report.\",\n",
    "    )\n",
    "    description: str = Field(\n",
    "        description=\"Brief overview of the main topics and concepts to be covered in this section.\",\n",
    "    )\n",
    "\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections: List[Section] = Field(\n",
    "        description=\"Sections of the report.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50e5cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the LLM with schema for structured output\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae988299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class AgentState(TypedDict):\n",
    "    topic: str  # Report topic\n",
    "    sections: list[Section]  # List of report sections\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]  # All workers write to this key in parallel\n",
    "    final_report: str  # Final report\n",
    "\n",
    "\n",
    "# Worker state\n",
    "class WorkerState(TypedDict):\n",
    "    section: Section\n",
    "    completed_sections: Annotated[list, operator.add]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c933752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nodes\n",
    "def orchestrator(state: AgentState):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "\n",
    "    # Generate queries\n",
    "    report_sections = planner.invoke(\n",
    "        [\n",
    "            SystemMessage(content=\"\"\"Generate a plan for the report. You must return ONLY a JSON object with the following structure:\n",
    "{\n",
    "  \"sections\": [\n",
    "    {\n",
    "      \"name\": \"Section Name\",\n",
    "      \"description\": \"Brief description of what this section covers\"\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "Create 4-6 sections for the report. Return ONLY the JSON, no other text or explanation.\"\"\"),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the report topic: {state['topic']}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "\n",
    "def llm_call(state: WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "\n",
    "    # Generate section\n",
    "    section = llm.invoke(\n",
    "        [\n",
    "            SystemMessage(\n",
    "                content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\"\n",
    "            ),\n",
    "            HumanMessage(\n",
    "                content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Write the updated section to completed sections\n",
    "    return {\"completed_sections\": [section.content]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d8c1e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesizer(state: AgentState):\n",
    "    \"\"\"Synthesize full report from sections\"\"\"\n",
    "\n",
    "    # List of completed sections\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "\n",
    "    # Format completed section to str to use as context for final sections\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "\n",
    "    return {\"final_report\": completed_report_sections}\n",
    "\n",
    "\n",
    "# Conditional edge function to create llm_call workers that each write a section of the report\n",
    "def assign_workers(state: AgentState):\n",
    "    \"\"\"Assign a worker to each section in the plan\"\"\"\n",
    "\n",
    "    # Kick off section writing in parallel via Send() API\n",
    "    return [Send(\"llm_call\", {\"section\": s}) for s in state[\"sections\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8837e6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a7b145b740>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build workflow\n",
    "orchestrator_worker_builder = StateGraph(AgentState)\n",
    "\n",
    "# Add the nodes\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9406d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x2a7b145b740>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add edges to connect nodes\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "\n",
    "orchestrator_worker_builder.add_conditional_edges(\n",
    "    source=\"orchestrator\",\n",
    "    path=assign_workers,\n",
    "    path_map=[\"llm_call\"]\n",
    ")\n",
    "\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b1458e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x000002A7B25553D0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the workflow\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "orchestrator_worker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48c26271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "response = orchestrator_worker.invoke(\n",
    "    {\"topic\": \"Create a report on LLM scaling laws\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a2d4623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Introduction to LLM Scaling Laws\n",
       "\n",
       "Scaling laws are empirical principles in artificial intelligence that describe predictable relationships between model size, dataset size, compute resources, and performance metrics in large language models (LLMs). As models and training data increase, these laws help forecast improvements in capabilities such as accuracy, fluency, and reasoning, enabling more efficient AI development.\n",
       "\n",
       "The significance of scaling laws lies in their ability to guide resource allocation, reduce trial-and-error in model design, and set benchmarks for progress in natural language processing. They underscore the trend that larger-scale training often leads to emergent abilities, pushing the boundaries of what AI can achieve.\n",
       "\n",
       "Key historical milestones include:\n",
       "\n",
       "- **2017-2018**: Early observations in deep learning, such as the \"Neural Scaling Laws\" paper by Hestness et al., which established foundational scaling trends across various domains.\n",
       "- **2020**: OpenAI's landmark paper \"Scaling Laws for Neural Language Models\" by Kaplan et al., which quantified scaling relationships specifically for LLMs, showing performance improvements with model parameters, data, and compute.\n",
       "- **2022**: DeepMind's \"Chinchilla\" research, which refined scaling laws by emphasizing optimal data-to-model size ratios, leading to more efficient training of models like Chinchilla itself.\n",
       "- **2023**: Continued validation and extensions, such as work on \"Emergent Abilities of Large Language Models,\" highlighting how scaling enables new functionalities not present in smaller models.\n",
       "\n",
       "---\n",
       "\n",
       "## Key Mathematical Foundations\n",
       "\n",
       "The scaling behavior of neural networks is characterized by power-law relationships across three critical resources: computational budget, dataset size, and model parameters. These relationships provide a quantitative framework to predict how performance improves as these resources are increased.\n",
       "\n",
       "### Compute Scaling\n",
       "\n",
       "The relationship between computational budget (measured in FLOPs, \\( C \\)) and model performance (loss, \\( L \\)) follows a power-law of the form:\n",
       "\n",
       "\\[\n",
       "L(C) \\propto C^{-\\alpha_C}\n",
       "\\]\n",
       "\n",
       "Where \\( \\alpha_C \\) is the scaling exponent for compute. Empirical studies (e.g., from OpenAI's Scaling Laws) have found this exponent to be roughly \\( \\alpha_C \\approx -0.05 \\) to \\( -0.07 \\), indicating that a significant increase in compute is required for a modest reduction in loss. The optimal allocation strategy, when scaling compute, is often to increase model size (\\( N \\)) and dataset size (\\( D \\)) in balance:\n",
       "\n",
       "\\[\n",
       "C \\propto N D\n",
       "\\]\n",
       "\n",
       "### Data Scaling\n",
       "\n",
       "The relationship between dataset size (\\( D \\)) and loss is also a power-law:\n",
       "\n",
       "\\[\n",
       "L(D) \\propto D^{-\\alpha_D}\n",
       "\\]\n",
       "\n",
       "The exponent \\( \\alpha_D \\) is typically found to be in the range of \\( -0.2 \\) to \\( -0.3 \\). This is a steeper scaling than compute, implying that increasing the amount of high-quality training data is highly effective for reducing loss, though subject to diminishing returns once the dataset becomes sufficiently large.\n",
       "\n",
       "### Parameter Scaling\n",
       "\n",
       "For model size, measured by the number of non-embedding parameters (\\( N \\)), the scaling law is:\n",
       "\n",
       "\\[\n",
       "L(N) \\propto N^{-\\alpha_N}\n",
       "\\]\n",
       "\n",
       "The exponent \\( \\alpha_N \\) is observed to be around \\( -0.07 \\) to \\( -0.09 \\). This relatively shallow slope indicates that simply increasing the number of parameters yields diminishing performance gains unless accompanied by proportional increases in data and compute to avoid underfitting.\n",
       "\n",
       "### The Joint Scaling Law\n",
       "\n",
       "When scaling all three factors optimally, the overall performance can be modeled by a joint power-law:\n",
       "\n",
       "\\[\n",
       "L(N, D, C) \\approx \\left( \\frac{N_c}{N} \\right)^{\\alpha_N} + \\left( \\frac{D_c}{D} \\right)^{\\alpha_D} + \\left( \\frac{C_c}{C} \\right)^{\\alpha_C}\n",
       "\\]\n",
       "\n",
       "Where \\( N_c \\), \\( D_c \\), and \\( C_c \\) are constants representing critical thresholds. In practice, these scaling relationships are interdependent. For optimal performance, the model should not be under-trained (insufficient compute/ data for a given size) or over-sized (too many parameters for the available data). The most efficient scaling occurs when these resources are increased in a balanced way, governed by the ratios of their respective exponents.\n",
       "\n",
       "---\n",
       "\n",
       "### Key Seminal Studies\n",
       "\n",
       "**1. Kaplan et al., \"Scaling Laws for Neural Language Models\" (2020)**\n",
       "\n",
       "This landmark paper systematically demonstrated the relationship between model scale (parameters, dataset size, and compute budget) and performance. The core finding was a power-law relationship: test loss decreases predictably as a power of the amount of compute used for training. The research established that increasing model parameters, data, and compute in concert leads to smooth, predictable improvements in performance, forming the foundational \"neural scaling laws.\"\n",
       "\n",
       "**2. Hoffmann et al., \"Training Compute-Optimal Large Language Models\" (Chinchilla, 2022)**\n",
       "\n",
       "The Chinchilla paper challenged the sheer parameter-centric scaling of models like GPT-3. It demonstrated that for a given compute budget, performance is optimized by training a model with *fewer* parameters but on *significantly more data*. The key empirical result was a 70 billion parameter model trained on 1.4 trillion tokens outperformed a 280 billion parameter model (Gopher) trained on 300 billion tokens. This established a crucial refinement to scaling laws: optimal performance requires balancing model size and dataset size.\n",
       "\n",
       "**3. Wei et al., \"Emergent Abilities of Large Language Models\" (2022)**\n",
       "\n",
       "This study documented that certain capabilities are not present in smaller models and appear abruptly as a function of scale. These \"emergent abilities,\" such as multi-step arithmetic, reasoning, and instruction following, were shown to be unpredictable via simple extrapolation from smaller models. This evidence highlighted that scaling doesn't just produce quantitative improvements but can lead to qualitative, phase-change-like leaps in model capability.\n",
       "\n",
       "### Analysis of Experimental Evidence\n",
       "\n",
       "**The Power-Law Relationship:** The primary empirical evidence from Kaplan et al. and subsequent work shows that the test loss (cross-entropy) scales as a power-law function of three key variables: the number of model parameters (N), the size of the training dataset (D), and the amount of compute used for training (C). This relationship is remarkably consistent across model architectures, suggesting it is a fundamental property of large-scale statistical estimation.\n",
       "\n",
       "**The Compute-Optimal Frontier:** The Chinchilla study provided concrete evidence that previous scaling efforts were data-starved. It established an optimal scaling law where the model size (N) and the dataset size (D) should be scaled proportionally (approximately N ∝ D) for a fixed compute budget. The empirical results showed that violating this balance—by over-investing in parameters—leads to suboptimal performance.\n",
       "\n",
       "**Predictable vs. Emergent Behavior:** The evidence points to a dual nature of scaling. For core metrics like loss, scaling is highly predictable. However, for specific tasks (especially those requiring reasoning), performance can exhibit sharp, emergent transitions. This suggests that while the *base capability* scales smoothly, the *expression* of complex skills has threshold effects, making the full impact of scaling difficult to forecast precisely from small-scale experiments.\n",
       "\n",
       "---\n",
       "\n",
       "# Practical Implications for Model Training\n",
       "\n",
       "## Optimizing Resource Allocation\n",
       "\n",
       "Scaling laws suggest that model performance improves predictably with increased compute, data, and model size. To optimize resource allocation:\n",
       "\n",
       "*   **Budget-Based Scaling:** First, determine your total compute budget (e.g., GPU hours). The laws indicate that this budget should be allocated in a balanced way. A common approach is the \"Chinchilla-optimal\" scaling: for every doubling of model size (parameters), the training dataset size should also be doubled. Avoid the pitfall of training a large model on a relatively small dataset.\n",
       "*   **Prioritize Data Quality:** When scaling the dataset, prioritize quality and diversity over sheer quantity. Cleaning and deduplicating data can be more cost-effective than gathering a larger volume of noisy data. Allocate resources for rigorous data curation.\n",
       "*   **Efficient Hardware Utilization:** Invest in hardware and software infrastructure that maximizes utilization (e.g., high-performance interconnects, efficient data loaders). Wasted compute cycles due to bottlenecks are a direct loss against your scaling budget.\n",
       "\n",
       "## Informing Model Architecture Decisions\n",
       "\n",
       "Scaling laws provide a quantitative framework for making architectural choices.\n",
       "\n",
       "*   **Parameter Count vs. Depth/Width:** While total parameter count is a key variable, scaling laws are largely agnostic to the specific arrangement of those parameters (depth vs. width). This provides flexibility. However, architectural innovations that improve training efficiency (e.g., better activation functions, normalization schemes) effectively give you \"more performance per parameter,\" allowing you to achieve a target performance level with a smaller, cheaper model.\n",
       "* **Testing Architectural Variants:** When evaluating a new architectural component (e.g., a new attention mechanism), the most effective test is to compare the scaling curves. Train a baseline model and the new model at several different scales (small, medium, large). If the new architecture's curve lies above the baseline's, it is a genuinely more efficient architecture.\n",
       "\n",
       "## Guiding Training Strategies\n",
       "\n",
       "Training dynamics are deeply intertwined with scale.\n",
       "\n",
       "*   **Learning Rate and Batch Size Scaling:** As model size and batch size increase, the optimal learning rate often changes. Use learning rate scaling rules (e.g., linear or square root scaling with batch size) as a starting point for hyperparameter tuning at larger scales. Automated hyperparameter optimization becomes increasingly valuable for large-scale training runs.\n",
       "* **Optimal Training Duration:** Scaling laws imply there is an optimal number of training steps for a given model and dataset size. Under-training leaves performance on the table, while over-training wastes compute without significant gains. Use validation loss curves to identify the point of diminishing returns.\n",
       "* **Regularization and Generalization:** Larger models have a greater capacity to overfit. As you scale up, the role of regularization (e.g., dropout, weight decay) may need to be re-evaluated. Often, appropriately scaled models trained on massive datasets exhibit less overfitting, potentially reducing the need for strong regularization.\n",
       "\n",
       "---\n",
       "\n",
       "# Limitations and Current Challenges\n",
       "\n",
       "## Scaling Law Assumptions\n",
       "\n",
       "Existing scaling laws, which forecast performance improvements of AI models as a function of parameters, compute, and data, are based on several critical and potentially limiting assumptions. They often assume:\n",
       "\n",
       "- **Smoothly scalable architectures**: That current neural network designs (e.g., the transformer) will continue to scale effectively without fundamental bottlenecks.\n",
       "- **Unbounded data availability**: That high-quality, novel training data will continue to be available in the quantities required, ignoring potential exhaustion of public data sources.\n",
       "- **Linear relationship with compute**: That performance improvements can be \"bought\" with more computational power, which may not hold if algorithmic efficiency plateaus or new, unforeseen physical limits are reached.\n",
       "- **Homogeneous hardware improvements**: That hardware scaling (e.g., Moore's Law) will continue at a predictable pace, which is becoming increasingly challenging.\n",
       "\n",
       "These assumptions are being tested as models grow, and deviations could invalidate current projections, leading to a potential performance plateau.\n",
       "\n",
       "## Emergent Constraints\n",
       "\n",
       "As the field of large-scale AI progresses, several emergent constraints are becoming prominent:\n",
       "\n",
       "- **Environmental and Economic Costs**: The energy consumption and carbon footprint of training and running massive models are significant and increasingly subject to public and regulatory scrutiny. The sheer economic cost of training (e.g., tens or hundreds of millions of dollars) creates a high barrier to entry, potentially stifling innovation and concentrating power.\n",
       "- **Data Scarcity and Quality**: Models are approaching the limits of publicly available, high-quality text and image data. The push for more data is leading to increased use of synthetic data or lower-quality web-scraped content, which may introduce biases, errors, and lower overall capability gains.\n",
       "- **Interpretability and Safety**: The complexity and \"black box\" nature of models with trillions of parameters make it extremely difficult to ensure they are safe, aligned with human values, and free from harmful biases. This opacity is a fundamental constraint on deployment in high-stakes domains.\n",
       "- **Infrastructure Limits**: Pushing the boundaries of scale requires bespoke, massive-scale computing infrastructure (e.g., exaflop-class supercomputers). The engineering challenges of building and reliably operating such systems at the extreme edge of current technology are non-trivial and act as a practical brake on scaling.\n",
       "\n",
       "## Unresolved Questions\n",
       "\n",
       "Several profound questions remain unresolved and are central to the future trajectory of the field:\n",
       "\n",
       "1.  **Is scale sufficient for general intelligence?** Will simply making models bigger and training them on more data inevitably lead to artificial general intelligence (AGI), or are fundamental architectural or algorithmic breakthroughs required?\n",
       "2.  **What are the fundamental limits of the transformer architecture?** Is the transformer the final foundational model for language, or will it be superseded by a more efficient or capable architecture as scaling continues?\n",
       "3.  **How can we reliably evaluate true understanding and reasoning?** Current benchmarks are often solvable through pattern recognition and memorization. How do we create evaluations that truly measure deep understanding, reasoning, and the ability to generalize to novel situations?\n",
       "4.  **What are the long-term societal impacts?** How will economies and societies adapt to the widespread deployment of highly capable AI? The long-term implications for employment, creativity, information integrity, and geopolitical stability are largely unknown.\n",
       "5.  **How do we achieve robust alignment and control?** We lack a proven, scalable methodology for ensuring that superhuman AI systems reliably and safely pursue complex goals specified by humans.\n",
       "\n",
       "---\n",
       "\n",
       "# Future Directions and Scaling Frontiers\n",
       "\n",
       "## Next-Generation Scaling Approaches\n",
       "\n",
       "As artificial intelligence systems grow in complexity and application breadth, advancing scaling methodologies becomes paramount. The next evolutionary steps focus on two synergistic frontiers: multimodal integration and algorithmic efficiency.\n",
       "\n",
       "### Multimodal Scaling\n",
       "\n",
       "The future of scalable AI lies in transcending unimodal limitations by harnessing interconnected data types. Multimodal systems combine text, images, audio, video, and sensor data to create more holistic and context-aware models. Key developments in this area include:\n",
       "\n",
       "- **Cross-Modal Alignment:** Creating unified embedding spaces where representations from different modalities are semantically aligned, enabling seamless translation and reasoning across data types.\n",
       "- **Fusion Architectures:** Designing novel neural network topologies that effectively merge features from diverse inputs, such as transformer-based models with parallel streams for each modality.\n",
       "- **Generalist Models:** Moving beyond specialized systems towards single, large-scale models capable of processing and generating multiple types of content, reducing deployment overhead and improving coherence.\n",
       "\n",
       "### Efficiency Optimizations\n",
       "\n",
       "As model sizes and computational demands surge, sustainable scaling necessitates a sharp focus on efficiency. Innovations aim to maintain or enhance performance while drastically reducing resource consumption.\n",
       "\n",
       "- **Sparse Models:** Leveraging techniques like mixture-of-experts (MoE) architectures, where only subsets of a model activate for a given input, effectively increasing capacity without proportionally increasing computation.\n",
       "- **Quantization and Compression:** Developing advanced methods to represent model parameters with lower precision (e.g., 8-bit or 4-bit integers) and pruning redundant weights, enabling faster inference and reduced memory footprint.\n",
       "- **Hardware-Software Co-Design:** Creating specialized accelerators (e.g., TPUs, neuromorphic chips) and algorithms optimized for them, targeting energy efficiency and high-throughput processing.\n",
       "- **Dynamic Computation:** Implementing adaptive mechanisms that allocate computational resources based on input complexity, allowing simpler queries to be processed faster while reserving full capacity for harder tasks.\n",
       "\n",
       "The interplay between multimodal capabilities and efficiency gains will define the next wave of AI progress, enabling more powerful, versatile, and accessible intelligent systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bd4f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aefa44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LANGGRAPH_TUT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
